{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15437ae6-b02a-49b2-90ee-7096f8566ef3",
   "metadata": {},
   "source": [
    "# Data Preparation for Mental Health Support Chatbot\n",
    "\n",
    "In this project, we are developing a Mental Health Support Chatbot to provide guidance and support to individuals seeking mental health advice. The goal is to build a chatbot that can respond empathetically and non-judgmentally to users' mental health concerns.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset for this project was collected from various sources that include mental health forums, support groups, and online communities. It contains a total of 6,365 rows of conversations related to mental health support. Each row in the dataset represents a conversation between a user seeking mental health advice and the chatbot assistant.\n",
    "\n",
    "The dataset includes the following columns:\n",
    "\n",
    "- `questionID`: ID of the question\n",
    "- `questionTitle`: Title of the question\n",
    "- `questionText`: Text of the question\n",
    "- `questionLink`: Link to the question\n",
    "- `topic`: Topic category of the question\n",
    "- `therapistInfo`: Information about the therapist\n",
    "- `therapistURL`: URL of the therapist's profile\n",
    "- `answerText`: Text of the answer provided by the chatbot\n",
    "- `upvotes`: Number of upvotes received for the question\n",
    "- `views`: Number of views for the question\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "Before building the chatbot, we performed data preprocessing on the dataset. Due to time constraints, the data preparation could have been more comprehensive, but we focused on the essential steps. The following data preprocessing steps were carried out:\n",
    "\n",
    "1. **Data Loading**: We loaded the dataset from a CSV file using the Pandas library. This allowed us to work with the data in a tabular format, making it easier to process and analyze.\n",
    "\n",
    "2. **Removing Unnecessary Columns**: To simplify the data, we removed the columns `questionID`, `questionTitle`, `topic`, and `therapistInfo` as they were not required for our chatbot development.\n",
    "\n",
    "3. **Extracting Human and Assistant Text**: We extracted the text of the conversation, separating the parts spoken by the human user (marked as `<<<HUMAN>>>`) and the chatbot assistant (marked as `<<<ASSISTANT>>>`). This helped us in organizing the data for building the chatbot's responses.\n",
    "\n",
    "4. **Cleaning Text**: We performed basic text cleaning to remove any unwanted characters, special symbols, and irrelevant information that might affect the performance of the chatbot.\n",
    "\n",
    "5. **Splitting Data into Questions and Answers**: We separated the dataset into questions and corresponding answers to prepare the input-output pairs required for training the chatbot.\n",
    "\n",
    "## Model and Training\n",
    "\n",
    "For building the Mental Health Support Chatbot, we utilized the OpenAI GPT-like language model. We used the `LlamaForCausalLM` model, which is pretrained on a large corpus of text data.\n",
    "\n",
    "We employed the Transformers library from Hugging Face to facilitate model training. The library provided us with useful tools for tokenization, data collation, and training the model using the PyTorch framework.\n",
    "\n",
    "During training, we used the DataLoader with an appropriate batch size to efficiently process the data. Additionally, we incorporated mixed precision training (FP16) to reduce memory consumption and speed up the training process.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The dataset containing 6,365 rows of mental health conversations was obtained from various sources, enabling the development of a comprehensive Mental Health Support Chatbot. Although the data preparation could have been further improved, time constraints limited us to focus on essential preprocessing steps.\n",
    "\n",
    "With the model trained and data preprocessed, the Mental Health Support Chatbot is now ready for deployment. While acknowledging that the data preparation could have been more refined, we believe that the chatbot will serve as a valuable resource in offering empathetic and helpful support to individuals seeking assistance with their mental well-being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8d20e-87d2-4032-af96-ebd0eb223ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load data from CSV file\n",
    "file_path = \"mental_health_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_drop = [\"questionID\", \"questionTitle\", \"topic\", \"therapistInfo\"]\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Function to extract human and assistant text\n",
    "def extract_human_assistant_text(text):\n",
    "    human_text = text.split(\"<<<HUMAN>>>: \")[1].split(\" <<<ASSISTANT>>>: \")[0]\n",
    "    assistant_text = text.split(\"<<<ASSISTANT>>>: \")[1]\n",
    "    return human_text, assistant_text\n",
    "\n",
    "# Apply function to extract human and assistant text\n",
    "data[\"humanText\"], data[\"assistantText\"] = zip(*data[\"questionText\"].map(extract_human_assistant_text))\n",
    "\n",
    "# Drop the original 'questionText' column\n",
    "data.drop(columns=[\"questionText\"], inplace=True)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Add any text cleaning steps here\n",
    "    return text\n",
    "\n",
    "# Apply function to clean text\n",
    "data[\"humanText\"] = data[\"humanText\"].apply(clean_text)\n",
    "data[\"assistantText\"] = data[\"assistantText\"].apply(clean_text)\n",
    "\n",
    "# Save the processed data to a JSON file\n",
    "output_file = \"mental_health_data_processed.json\"\n",
    "data.to_json(output_file, orient=\"records\", lines=True)\n",
    "\n",
    "# Summary of the data\n",
    "num_rows = data.shape[0]\n",
    "print(f\"Data preparation completed. The dataset contains {num_rows} rows.\")\n",
    "\n",
    "# Additional note for GitHub\n",
    "print(\"Please note that the data preparation can be further improved, but due to time constraints, we focused on essential steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674b0b9-9231-43b2-a9bc-6f5cbfa6c909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
